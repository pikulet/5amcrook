{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/gambling-info-theory","result":{"data":{"markdownRemark":{"id":"727b397e-ae09-53a1-83bb-55181d9b99e6","html":"<h3 id=\"information-theory\" style=\"position:relative;\"><a href=\"#information-theory\" aria-label=\"information theory permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Information Theory</h3>\n<p>Information Theory is a field concerned with how much information certain events can reveal. In general, events with a lower probability have more information. The commonly-used measure of “entropy” refers to the average amount of information across all events. </p>\n<p>If variable A can hold the values of a1 with probability 0 and a2 with probability 1, then while the event of a1 occurring gives a lot of information, the event of a2 occurring gives no information. Hence, the average entropy is really 0. </p>\n<p>On the other hand, if variable B can be b1 or b2 with equal probability, then it would have the highest average entropy.</p>\n<h3 id=\"usefulness-of-information-theory\" style=\"position:relative;\"><a href=\"#usefulness-of-information-theory\" aria-label=\"usefulness of information theory permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Usefulness of Information Theory</h3>\n<p>Information Theory is relevant to a lot of fields, but I will explain two bigs ones - Machine Learning and Cryptography.</p>\n<h4 id=\"machine-learning\" style=\"position:relative;\"><a href=\"#machine-learning\" aria-label=\"machine learning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Machine Learning</h4>\n<p>Consider an algorithm that aims to classify if an image is a cat or dog. The result is stored in the random variable X. Without any prior information, the amount of information in X is exactly the same as the distribution of cat and dog images on the internet. </p>\n<p>However, if we have some training data, then we can instead consider the conditional entropy of the prediction outcome X given the data. An algorithm that works well would aim to minimise this entropy in X using more relevant training data and methods.</p>\n<p>By measuring the uncertainty in the prediction outcome X, we can better devise algorithms that aim to reduce the uncertainty in X.</p>\n<h4 id=\"cryptography\" style=\"position:relative;\"><a href=\"#cryptography\" aria-label=\"cryptography permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Cryptography</h4>\n<p>Encoding and decoding bits is at the core of information theory. Given a message like “hello, welcome to this site”. How many bits do we need to encode it such that</p>\n<ul>\n<li>the receiver can correctly decode the message</li>\n<li>the message is resistant to errors in the channel, such as incorrectly-transmitted Morse code?</li>\n</ul>\n<p>Information Theory aims to prove fundamental limits in what can be achieved in real-world encoding. We can apply the knowledge learnt to the encoding of different media, from text to images and even audio.</p>\n<h3 id=\"the-project\" style=\"position:relative;\"><a href=\"#the-project\" aria-label=\"the project permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The Project</h3>\n<p>For my term project in the module, I did a report on Gambling and Data Compression, from Cover and Thomas’ chapter. In short, an information-theoretic perspective was used to frame the horse race gambling problem. The results from this analysis were then applied in the prediction of English language text. The predictor is basically a horse better, making bets on the next letter that will occur in a given sequence of letters. Hence, if the gambler can make good predictions and earn a lot of wealth, the predictor can also make good predictions for the english language.</p>\n<p>The report is targeted at readers familiar with basic notations in Information Theory. If you’d like to learn more about the topic, I can direct you to some great resources. Interested readers can read the full analysis <a href=\"/gambling.pdf\">here</a>.</p>","fields":{"slug":"/posts/gambling-info-theory","tagSlugs":["/tag/information-theory/"]},"frontmatter":{"date":"2020-04-30","description":"Using an information-theoretic perspective to understand horse races.","tags":["information theory"],"title":"Gambling and Data Compression"}}},"pageContext":{"slug":"/posts/gambling-info-theory"}}}